{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline-crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LFBJC/projeto-ri-lfbjc-e-vms5/blob/master/baseline_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpa2Zj0onV6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO\n",
        "ALGO ERRADO NAS REQUISIÇÕES (NÃO QUER NEM PRINTAR O CÓDIGO DE STATUS, Nem mesmo try except)\n",
        "TRATAR CASO DE HAVEREM OUTROS AGENTES DEPOIS D *\"\"\"\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from threading import Thread"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esiYGTdjncV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visit(url,visited_URLs,to_be_visited,contents,forbidden):\n",
        "  protocol, url_wout_protocol = url.split('://')\n",
        "  pure_site_url=''\n",
        "  if(url_wout_protocol.find('/')!=-1):\n",
        "    pure_site_url = protocol+'://'+url_wout_protocol[0:url_wout_protocol.find('/')+1]\n",
        "  else:\n",
        "    pure_site_url = protocol+'://'+url_wout_protocol+'/'\n",
        "  URL_robots = pure_site_url + \"robots.txt\"\n",
        "  URL_node = url\n",
        "  visited_URLs.append(URL_node);\n",
        "  filter(lambda a: a != URL_node, to_be_visited)\n",
        "  try:\n",
        "    req_robots = requests.get(url = URL_robots)\n",
        "  except name_error:\n",
        "    print('houve um erro '+name_error)\n",
        "  print(URL_robots)\n",
        "  robots = req_robots.content.decode('utf-8')\n",
        "  print(URL_robots)\n",
        "  matchRobots = re.search('User-agent: \\*.*',robots)\n",
        "  if(matchRobots == None):\n",
        "    print('o site ' + URL_robots + ' deu erro!!!!!!!!!!!!!')\n",
        "    return (visited_URLs,to_be_visited,contents,forbidden)\n",
        "  locally_forbidden = [s[re.search('Disallow: ',s).end():] for s in re.findall('Disallow: .*',robots[matchRobots.start():])]\n",
        "  locally_forbidden = map(lambda s: pure_site_url[:-1]+s, locally_forbidden)\n",
        "  forbidden.extend(locally_forbidden)\n",
        "  node_content = requests.get(url = URL_node).content\n",
        "  f = open('sites baixados/' + URL_node + '.html','w+')\n",
        "  contents.append(node_content)\n",
        "  return (visited_URLs,to_be_visited,contents,forbidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iavqO3JnqCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_neibourhood(url,to_be_visited,contents,forbidden):\n",
        "  print('encontrando vizinhança de '+ url)\n",
        "  soup = BeautifulSoup(contents[-1])\n",
        "  for anchor in soup.findAll('a'):\n",
        "    if('href' in anchor.attrs): #checa se realmente é um link pq algumas ancoras estao sem links ou chamam scripts\n",
        "      href = anchor.attrs['href']\n",
        "      if((href[:href.find(':')]=='http') or (href[:href.find(':')]=='https')):\n",
        "         to_be_visited.append(href)\n",
        "    filter(lambda l: all(l!=href for href in visited_URLs), to_be_visited);#retira endereços visitados\n",
        "    filter(lambda href: all(href!=s for s in forbidden),to_be_visited);#retira endereços proibidos\n",
        "    return (to_be_visited,contents,forbidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQmxrFyTntoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline(url,visited_URLs,to_be_visited,contents,forbidden):\n",
        "  visited_URLs,to_be_visited,contents,forbidden = visit(url,visited_URLs,to_be_visited,contents,forbidden);\n",
        "  to_be_visited,contents,forbidden = find_neibourhood(url,to_be_visited,contents,forbidden)\n",
        "  for ref in to_be_visited:\n",
        "    visited_URLs,to_be_visited,contents,forbidden = visit(ref,visited_URLs,to_be_visited,contents,forbidden);\n",
        "    to_be_visited,contents,forbidden = find_neibourhood(ref,to_be_visited,contents,forbidden)\n",
        "    print(to_be_visited)\n",
        "    time.sleep(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDXYxZyYkKAK",
        "colab_type": "code",
        "outputId": "6246f9ef-46a6-4f38-dd83-942a21468708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "URLs = [\"https://pe.olx.com.br/imoveis\",\"http://www.expoimovel.com/recife/\", \"https://www.vivareal.com.br/\",\n",
        "        \"https://www.imovelweb.com.br/\",\"https://www.chavesnamao.com.br/\",\"https://www.trueimoveis.com.br/imoveis/\",\n",
        "        \"https://www.newville.com.br/imoveis/\",\"https://imoveis.trovit.com.br/\",\"https://www.mercadolivre.com.br/imoveis\",\n",
        "        \"https://ancoraimobiliaria.com.br/\", \"https://www.zapimoveis.com.br/\",\"https://apsa.com.br/imoveis\",\n",
        "        \"https://www.paulomiranda.com.br/imoveis/\",\"https://www.gedeaoimoveis.com.br/\"];\n",
        "class Th(Thread):\n",
        "  def __init__ (self, url):\n",
        "                      Thread.__init__(self)\n",
        "                      self.url = url\n",
        "  def run(self):\n",
        "    baseline(url=self.url,visited_URLs=[],to_be_visited = [],contents = [],forbidden = [])\n",
        "for url in URLs:\n",
        "#url = URLs[0]\n",
        "  th = Th(url);\n",
        "  th.start();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.6.16)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}